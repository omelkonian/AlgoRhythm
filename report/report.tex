\documentclass[11pt,a4paper]{article}

% Math
\usepackage{amssymb}

% Geometry
\usepackage{titling}
\setlength{\droptitle}{-7em}

% Timeline
\usepackage{chronosys}

% Code
\usepackage{minted}
\usemintedstyle{friendly}

% Tables
\usepackage{multicol}
\usepackage{multirow}
\usepackage{csquotes}
\usepackage{fullpage}

% Colors
\usepackage{xcolor, color, colortbl}
\colorlet{gray}{gray!70}
\colorlet{green}{green!50}
\definecolor{darkblue}{HTML}{1D577A}
\definecolor{rred}{HTML}{C03425}
\definecolor{darkgreen}{HTML}{8BB523}
\definecolor{ppurple}{HTML}{6B1B7F}
\definecolor{pblack}{HTML}{000000}
\definecolor{darkyellow}{HTML}{C0B225}

% Links
\usepackage{hyperref}
\definecolor{linkcolour}{rgb}{0,0.2,0.6}
\hypersetup{colorlinks,breaklinks,urlcolor=linkcolour,linkcolor=linkcolour,citecolor=blue}

% Title
\title{\textbf{\Large{AlgoRhythm: A Library for Algorithmic Music Composition} \\ \small{AFP: Final Report}}}
\author{\small{Joris ten Tusscher, Cas van der Rest, Orestis Melkonian}}
\date{}

% Macros
\newcommand{\site}[1]{\footnote{\url{#1}}}
\newcommand{\icode}[1]{\mintinline{haskell}{#1}}
\newcommand\todo[1]{\textcolor{red}{#1}}

\begin{document}
\maketitle

\section{Domain}
We set out to implement a Haskell library for algorithmic music composition.
\subsection{Algorithmic music composition}
The process of algorithmic music composition is mostly helpful as an additional aid to a composer's toolbox, allowing for generation of vast and highly diverse musical material. The composer's role then is to refine this material by identifying interesting segments and combine them in a tasteful, coherent whole.
\subsection{Music representation}
An immediate requirement to facilitate algorithmic music composition is the ability to concisely represent music. One could use the universal standard for musicians of western music, namely sheet scores. This representation, although incredibly helpful for real-time interpretation by human performers, is not fitted to computational manipulation. An appropriate representation would utilize the added expressiveness computation bestows upon us.

We refrain from taking music analysis into consideration, since it is outside the scopes of our library and could be independently performed at a separate stage of a composer's workflow. Hence, we do not provide any functionality  to pre-existing pieces of music.

\subsection{Music generation}
Apart from representing music, there is a need to randomly generate a variety of interesting musical pieces. Furthermore, one would want to have some control over the generation procedure, such as requiring the output of melodies respecting a specific structural rule.

Naturally, our library should output generated music in some common format. We chose to support common sheet notation in PDF and digital formatting in MIDI.

\subsection{Motivation}
This project is an homage to the late Paul Hudak, one of the creators of Haskell and professional jazz pianist. His work combined functional programming and computer music, which essentially gave birth to this particular line of research (for current work, see ICFP's \href{https://icfp17.sigplan.org/track/farm-2017-papers}{FARM} workshop). We were greatly influenced by his general datatype-oriented view of polymorphic temporal media \cite{temporal}, his music Haskell library \href{http://euterpea.com}{Euterpea} and his linguistic approach to music generation \cite{hudak}.

\section{Main Concepts \& Techniques}

\subsection{Representation DSL}
For representing music, we defined a highly-abstract \icode{Music} datatype, polymorphic to the specific elements of music manipulated through time. Hence, this datatype only specifies the operations invariant across all such specific representation, such as sequential/parallel composition of music and basic construction of music events or silence (of a certain duration), as shown below:
\begin{center}\begin{minipage}{0.5\textwidth}\begin{minted}[baselinestretch=1.1, fontsize=\small]{haskell}
data Music a = Music a :+: Music a
             | Music a :=: Music a
             | Note Duration a
             | Rest Duration
\end{minted}
\end{minipage}
\end{center}

Since our datatype is polymorphic, we can freely change abstraction levels of music representation (as can be witnessed by our applications). To allow for playback/rendering/exporting, we define a core representation \icode{MusicCore = Music (Pitch, Attributes)}, which every other abstraction should convert to.

Futhermore, we define common constants (e.g. scales, chords), useful types of music (e.g. \icode{Melody = Music Pitch}, \icode{Rhythm = Music ()}, \icode{Harmony = Music Chord}), as well as operations often used in algorithmic music composition, such as transposition, inversion, mirroring, scaling and repetition. These are implemented as typeclasses, since multiple music types could be manipulated in the same way (e.g. we could transpose both a \icode{Pitch} and a \icode{Scale}).

\subsection{Export functionality}
Export functionality is provided for both \texttt{.midi} files and \texttt{.ly} files, which can be converted to PDF using \href{https://lilypond.org}{Lilypond}.

\subsubsection{Musical Score}
Any value of type \icode{Music} can be converted to a Lilypond file representing a musical score using \icode{writeToLilypondFile "music.ly" m} functions, provided that \icode{m} is a member of the \icode{ToMusicCore} type class, meaning that it can be converted to the \icode{MusicCore} type. Assuming that the user has Lilypond installed on their system, the generated file can be converted into a PDF using \icode{lilypond music.ly}. An example of a generated score can be found in figure ?.
\subsubsection{MIDI files}
\todo{MIDI export}

\subsection{Generation}
\todo{monadic interface, selectors, etc...}
A simple, constraint based, monadic interface is included in order to allow a user to program their own music generators. This interface is defined as a combined \icode{State} and \icode{IO} monad: \icode{type GenericMusicGenerator st s a = StateT (st s) IO a}. We provide an instance of this interface to generate for the types used in the representation DSL, defined as \icode{type MusicGenerator s a = GenericMusicGenerator GenState s a}.

\subsubsection{Default generation state}
The \icode{GenState} datatype consists of several values of the type \icode{Entry s a}, where \icode{a} is the type of the value that is contained within the \icode{Entry}, and \icode{s} is the state that is kept when selecting entries from the \icode{GenState}. Each \icode{Entry} consists of three elements: \icode{values :: [(Weight, a)]}, \icode{constraints :: [a -> Bool]} and \icode{selector :: Selector s}. The \icode{values} can be used to generate certain values with a higher probability, \icode{constraints} allow the exclusion of certain values, and \icode{selector} describes how to select a value from this entry, where the \icode{Selector} type is defined as follows: \icode{type Selector s a = s -> [(Weight, a)] -> IO (a, s)}. Selection based upon previously generated values is thus possible, since \icode{Selector} is essentially a state monad.

\subsubsection{Composing generators}
An auxiliary data type \icode{Accessor} is defined, that describes how to get and set values in the generator state:
\begin{minted}[baselinestretch=1.1, fontsize=\small]{haskell}
data Accessor st s a = Accessor
  { getValue :: st s -> Entry s a
  , setValue :: Entry s a -> st s -> st s
  }
\end{minted}

The library functions for adding constraints and selectors are parameterized with an accessor, so that the right \icode{Entry} is manipulated.

\subsection{Grammars}
To accommodate a linguistic approach to music generation, we provide a DSL for defining generative grammars. These are similar to context-free grammars, with the addition of being probabilistic (i.e. allow assigning weights to individual rules), temporal (i.e. rules are parametric to duration) and node-sharing (i.e. allow repetition of generated symbols using \textit{let}-expressions).

For brevity's sake, we eschew a formal definition of our grammars, and immediately give the definitions of the corresponding Haskell datatypes. We also refer the reader to our \href{https://github.com/omelkonian/AlgoRhythm/blob/master/AlgoRhythm/src/Grammar}{grammar implementations} for examples.
\begin{minted}[baselinestretch=1.1, fontsize=\small]{haskell}
data Grammar meta a = Grammar { initial :: a, rules :: [Rule meta a] }
data Rule meta a = (a, Weight, Duration -> Bool) :-> (Duration -> Term meta a)
data Term meta a = Prim (a, Duration)                             -- primitive
                 | Term meta a :-: Term meta a                     -- sequence
                 | Aux Bool meta (Term meta a)          -- auxiliary modifiers
                 | Let (Term meta a) (forall b. Term () b -> Term () b) -- let
\end{minted}

Given an initial symbol and some initial value of type \icode{input}, the grammar expansion will begin rewriting by a randomly-picked activated rule, up to fixpoint. A rule's body can be a primitive terminal symbol of a certain duration, a sequence of terms, a term wrapped with auxiliary metadata which will be used in a post-processing step, or a let-expression of a term, which will share a rewritten term in the let's body (when fixpoint is reached).

The symbol type \icode{a} along with the metadata type \icode{meta} and the input type \icode{input} must be implement a way to strip off metadata (possibly converting to some more concrete representation. This is captured by the \textit{Expand} typeclass:
\begin{center}\begin{minipage}{0.6\textwidth}\begin{minted}[baselinestretch=1.1, fontsize=\small]{haskell}
class Expand input a meta b | input a meta -> b where
  expand :: input -> Term meta a -> IO (Term () b)
\end{minted}
\end{minipage}
\end{center}

When the aforementioned fixpoint is reached, we are left with metadata-enriched terms and \textit{let}-expressions. The expansion proceeds as follows: the auxililary metadata are stripped-off (by \icode{expand}) and the \textit{let}-expressions are unfolded. This is then trivially converted to our \icode{Music} datatype, since we only have to deal with primitive values (i.e. \icode{Notes}) and sequential composition (i.e. \icode{:+:}).

\subsection{Dynamic performance}
\todo{k-means clustering, etc...}


\section{Results}
\subsection{Music DSL}
To showcase the expressive power of our DSL for representing and manipulating music, we present a very short piece of code, which however employs a variery of operators (transposition, retrograde, time-scaling, delay) and generates a rather complex musical effect (\href{https://soundcloud.com/algo-rhythm-haskell/hypnotic}{audio}):
\begin{minted}[baselinestretch=1.1, fontsize=\small]{haskell}
hypnotic :: Melody
hypnotic = 2%5 *~ cascades :+: (cascades ><)
  where cascades = rep id      (%> sn) 2 cascade
        cascade  = rep (~> M3) (%> en) 5 run
        run      = rep (~> P4) (%> tn) 5 (D#3 <| tn)
        rep :: (Melody -> Melody) -> (Melody -> Melody) -> Int -> Melody -> Melody
        rep _ _ 0 _ = (0~~)
        rep f g n m = m :=: g (rep f g (n - 1) (f m))
\end{minted}

\subsection{Generation DSL}
The monadic interface allows for relatively easy manipulation of the generation process. There are a few ways in which a user may influence the way in which the music is generated. They include adding constraints, choosing selectors and setting the frequency/probability for individual elements.

\begin{minted}[baselinestretch=1.1, fontsize=\small]{haskell}
-- Add a new constraint (only quarter notes or faster)
addConstraint duration (<=(1%4))

-- Provide a custom selector
putSelector pitchClass mySelector

-- Provide a custom distribution for elements (favor higher octaves)
putOptions dynamics
  [ (0.01, PPPPP)
  , (0.05, PPPP)
  ...
  , (0.3, FFFF)
  ]
\end{minted}

An aggregate example where the interface is used to generate a melody in the C major scale over a G7 chord with a length of 4 whole notes is shown below:

\begin{minted}[baselinestretch=1.1, fontsize=\small]{haskell}
melodyInC :: MusicGenerator () MusicCore
melodyInC = do
  addConstraint pitchClass (inScale C major)
  options <- getOptions pitchClass
  rhythm  <- boundedRhythm (4 * wn) Medium
  -- Chord tones are 4 times as likely to be picked
  let options' = map
        (\(w, v) ->
          if v `elem` (instantiate G d7 :: [PitchClass])
            then (4 * w, v) else (w, v)) options
  -- set options and generate pitches
  putOptions pitchClass options'
  pitches <- replicateM (length rhythm) (pitchClass??)
  -- put everything together into a piece of music
  let fullPitches = (flip (<:) $ []) <$> (zipWith (#) pitches (repeat 4))
  let gmaj7 = (toMusicCore . chord .
        map (Note (4 * wn) . (flip (#)) 3)) (G =| d7)
  return $ gmaj7 :=: line (zipWith (<|) fullPitches rhythm)
\end{minted}

The monadic interface allows for easy implementation of simple heuristics, as demonstrated above. A more involved example of this can be found in the \icode{Generate.Applications.Diatonic} module, which generates melodies based on the idea that smaller intervals (steps and skips) between consecutive notes are more common than larger intervals (leaps).

\subsection{Grammars}
To showcase the generative power of our grammars, we implement grammars all basic elements of music, namely harmony, melody and rhythm. To save space, we only explain the intuition behind each grammar and refer the reader to the actual implementations.

\subsubsection{Tonal harmony [\href{https://github.com/omelkonian/AlgoRhythm/blob/master/AlgoRhythm/src/Grammar/TonalHarmony.hs}{code}]}
Harmonic structure is argued to exhibit recursion and hierarchical organization, hence fit nicely to our grammar framework. We directly implement the generative grammar proposed by Rohrmeier\cite{tonal}, which captures harmonies on diatonic cadences (i.e. movements between chords of a different degree in some scale). The grammar is based on a Schenkarian view of harmony\cite{schenker}, namely the abstraction of an elaborate chord progression as a single scale degree. For instance, the cadence V-I can be seen as a harmonic element based on the tonic (I).

The grammar rewrites basic scale degrees into more elaborate ones, making sure the produced chord progressions have appropriate durations. One important addition is that of \textit{key modulation}, where an auxiliary key wraps a grammar term and makes the generated sub-progression be interpreted in a transposed key from the original. This is handled particularly well by our \icode{Aux} terms and \icode{Expand} typeclass.

\subsubsection{Jazz melodic improvisation [\href{https://github.com/omelkonian/AlgoRhythm/blob/master/AlgoRhythm/src/Grammar/Melody.hs}{code}]}
For melodic improvisation, we implemented the context-free grammar behind the open-source educational tool Impro-Visor\cite{improvisor}, which aims to train novice musicians in jazz improvisation. The grammar consists of two-levels, one to produce an interesting rhythmic structure of a given length and one to fill these values with notes having a certain function against some harmonic context (e.g. chord tones, approach tones). However, these could be inhabited by many possible concrete pitches, therefore a stochastic post-processing step heuristically assigns theses values (based on pitch distance and preferred octaves). This grammar does not use the metadata-enrichment features of our grammars.

\subsubsection{Tabla rhythmic improvisation [\href{https://github.com/omelkonian/AlgoRhythm/blob/master/AlgoRhythm/src/Grammar/Tabla.hs}{code}]}
Last but not least, we implement a grammar for improvising rhythmic sequences for Tabla, an Indian percussion instrument. We base our implementation on a proposed grammar used in the expert system \textit{Bol Processor BL1}\cite{tabla}. The rules operate on a fixed number of syllables, since this is how Tabla music is written (i.e. each syllable corresponds to a particular stroke of the drum set).\vspace{-3mm}
\paragraph{Technical note} Since there is no standard MIDI encoding of tabla sounds, we had to provide a custom MIDI mapping from notes to percussion sounds. Interestingly, this was elegantly modelled by our \icode{ToMusicCore} typeclass, which provided the custom mapping.

\subsection{Dynamic performance}
\todo{combine with previous section?}

\subsection{Produced songs [\href{https://github.com/omelkonian/AlgoRhythm/blob/master/AlgoRhythm/app/Main.hs}{code}]} We accumulate our results in several musical pieces automatically generated by our library (up to instrument assignment). You can see the corresponding music scores \href{https://github.com/omelkonian/AlgoRhythm/tree/master/output}{here} and the audio files in AlgoRhythm's dedicated \href{https://soundcloud.com/algo-rhythm-haskell/sets}{Soundcloud account}.

\section{Reflection}
\todo{more type trickery}\\
Although the monadic interface works reasonably well for implementing simple heuristics, actually generating more complex pieces of music turned out to be quite hard. We think that this flaw is fundamental to our approach, as the \icode{GenericMusic} type is very \emph{flat}, in the sense that it is only aware of the most fundamental aspects of music, such as notes and durations. Since musical compositions often exhibit structure on many levels, generated music that only concerns about the smallest building blocks will not be very convincing.

As it turned out, the selected monadic approach does not scale well in order to incorporate these larger structures into it's output, which makes it a bit of a dead end for more complex purposes.

Regarding grammars, one problem we were not able to overcome, was implementing a grammar for jazz chord progressions, as proposed by Steedman\cite{jazzchords}, since it required context-sensitive features and that would make our grammar implementation overly complicated and necessarily slower.\\

\newpage
\bibliographystyle{ieeetr}
\bibliography{sources}

\end{document}
